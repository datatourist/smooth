---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
Под языковыми моделями как правило подразумевается вероятностное распределение строк, которое пытается предугадать как часто данная строка встречается в предложении: Например, для языковой модели, описывающей разговорный язык мы можем иметь p(привет) = 0.01, так как возможно в каждом сотом предложении человек произносит слово привет.
С другой стороны p(жопа енот красный олег) = 0, так как очень маловероятно, что кто-нибудь произнесёт эти фразы в повседневной жизни.

Чтобы сделать p(wi jwi1) осмысленным для i = 1, мы должны подставить в начале предложения специальный маркер "." То есть мы предполагаем, что w0 - это "." Аналогично, чтобы сумма вероятностей всех фраз была равна 1, необходимо поместить такой же токен в конец предложений.Например, чтобы посчитать p(John read a book) надо:
p(John read a book) = p(Johnj<bos>)p(readjJohn)p(ajread)p(bookja)p(<eos>jbook)
(без этого, сумма вероятностей всех строк заданой длины равна 1, а сумма вероятностей всех строк будет бесконечной)

Рассмотрим небольшой пример: наш тестовый корпус: 
(John read Moby Dick"; Mary read a different book"; She read a book by Cher")


Добавить чэйн рулээ оф пробабилити


Что мы будем делать со словами, которые есть в нашем словаре, но появляются в невиданном ранее контексте(например они появляются после слова, после которого до этого не появлялись). Чтобы предостеречь нашу модель от присваивания нулевой вероятности  мы должны как бы сбрить немного вероятности от более часто встречающихся событий и присваит эту вероятность событиям, которые мы никогда не видели.

Рассмотрим предложение Шэр читает книгу
p(Шэр читает книгу) = 0, очевидно, это недооценка вероятности, так как эта фраза должна иметь какую-то вероятнсть. Чтобы показать как важно чтобы у вероятностей не было нулевых значений, обратимся к важному (основному) применению языковых моделей - распознаванию речи.
В распознавании речи мы пытаемся найти прделожение s которое бы максимизировала p(sjA) = p(Ajs)p(s)p(A) для заданного акустического сигнала А. Если  p(s) ноль тогда p(s|A) тоже будет ноль и предложение s не будет рассматриваться  как транскрипция, не зависимо от того, насколько акустический сигнал будет однозначным.Таким образом, каждый раз когда такая фраза будет появляться в процессе распознавания речи, будет выводиться ошибка. Задание всем фразам не нулевую вероятность зацитит от таких ошибок.


*MLE присваивает нулевую вероятность всем нграммам не в корпусе
*Это слишком строго, так как есть очень много отличных нграмм, которые просто произошли вне корпуса
*сглаживание это путь для присваивания маленькой но не нулевой вероятности этим нграммам
*сглаживание называется также дискаунтинго, так как часть вероятности нграмм с большей вероятностью перераспределяется среди нулевых нграмм

С этой проблемой предлагает справиться сглаживание.Теримн сглаживание происходит из факта, что эта техника делает распределение более равномерным, поднимая нулевые вероятности вверх и высокие вероятности вниз. 
Оно как Робин Гуд: крадёт у богатых и раздаёт бедным.

Мы рассмотрим лишь два простейших варианта: сглаживание лапласа и метод желинека-мерсье

Сглаживание лапласа. Простейший путь - добавить один ко всем биграммам до того, как мы нормализуем их к вероятностям. Все counts которые были нулём теперь стали 1, которые были 1 стали 2 и так далее. Такое сглаживание так же называют добавь один сглаживание.
В более общем виде такое сглаживание добавляет один к каждому событию и затем нормализует знаменатель с V - размер словаря.

P
∗
Laplace(wn|wn−1) = C(wn−1wn) +1
C(wn−1) +V

где V - размер словаря

Такой тип сглаживания даёт довольно бедные результаты, с ним связано несколько проблем:
* Если сглаживание крадёт у богатых и раздаёт бедным
* то в случае Лапласа, оно крадёт слишком много
Улучшенный метод: Additive smoothing add-k smoothing
Смысл в том, что он крадёт меньше:
0 < k ≤ 1.

P
∗
Add-k(wn|wn−1) = C(wn−1wn) +k
C(wn−1) +kV


интерполяция Йелинека-мерсера:
Если мы пытаемся посчитать вероятность (wn|wn−2wn−1)
но у нас нет примеров триграммы  wn−2wn−1 мы можем взамен оценить её вероятность используя вероятность биграммы P(wn|wn−1), похожим образом, если у нас нет коунтс чтобы посчитать P(wn|wn−1) мы берём униграмму P(wn) 


В простейшей интерполяции мы совмещаем n-граммы разного порядка линейно интерполируя все модели. Таким образом мы оцениваем вероятность триграммы
комбинируя униграмные биграмные и триграмные вероятности, взвешенные по альфа, так, чтобы сумма альф равнялась единице.

Значения лямды итой подбираются таким образом, чтобы оптимизировать перплексии на валидационной (проверочной) выборке

Оценка эффективности модели

Перплексия языковой модели на тестовой выборке - это обратная вероятность 










